{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gptransfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertwujj/gptransfer/blob/master/gptransfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ED518AlnbHEb",
        "colab_type": "code",
        "outputId": "695e3e69-ae75-405b-e568-cbf36d3de2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "from os.path import join\n",
        "from google.colab import drive\n",
        "\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT, force_remount=True)\n",
        "\n",
        "PROJ = \"My Drive/ColabExperiments/\"\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "%cd \"{PROJECT_PATH}\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/ColabExperiments\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LqeFzUodlIjT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3eKEsObUIko8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tjHg2j9jkpVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from importlib.machinery import SourceFileLoader\n",
        "model = SourceFileLoader('model', 'gpt-2/src/model.py').load_module()\n",
        "encoder = SourceFileLoader('encoder', 'gpt-2/src/encoder.py').load_module()\n",
        "sample = SourceFileLoader('sample', 'gpt-2/src/sample.py').load_module()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CgrVJpSXOvUa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "GkMT9lW_ICKA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from itertools import islice\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eS5nl9Ry7aKr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "3y31vSJDdqbU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from model import block, shape_list, positions_for, norm, default_hparams\n",
        "\n",
        "\n",
        "\n",
        "def custom_model(hparams, X, past=None, scope='model', reuse=False):\n",
        "\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        results = {}\n",
        "        batch, sequence = shape_list(X)\n",
        "\n",
        "        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
        "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
        "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
        "        \n",
        "        # Transformer\n",
        "        presents = []\n",
        "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
        "        assert len(pasts) == hparams.n_layer\n",
        "        for layer, past in enumerate(islice(pasts, hparams.n_layer)):\n",
        "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
        "            presents.append(present)\n",
        "        results['present'] = tf.stack(presents, axis=1)\n",
        "        h = norm(h, 'ln_f')\n",
        "\n",
        "        # Language model loss.  Do tokens <n predict token n?\n",
        "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
        "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
        "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
        "        results['logits'] = logits\n",
        "\n",
        "        results['h'] = h\n",
        "        return results\n",
        " \n",
        "def small_model(hparams, X, **kwargs):\n",
        "    hparams.n_layer = 5\n",
        "    return custom_model(hparams, X, **kwargs)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yzC4ZoGYYOjw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "def read_amazon_kaggle(filename, lines=None):\n",
        "    \n",
        "    with open(filename) as f:\n",
        "        label_x = [(1 if line[9] == '2' else -1, line[10:].strip()) for line in f]\n",
        "        label_x = random.sample(label_x, lines)\n",
        "        label, x = zip(*label_x)\n",
        "        return label, x\n",
        "      \n",
        "\n",
        "labels, dataX = read_amazon_kaggle('gptransfer/data/amazonreviews/train.ft.txt', 100000)\n",
        "\n",
        "\n",
        "size = (len(labels) // batch_size) * batch_size\n",
        "labels = np.asarray([[elt] for elt in labels[:size]])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zRkoJ-Sip79",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from encoder import Encoder\n",
        "def get_encoder(model_name):\n",
        "    with open(os.path.join('gpt-2/models', model_name, 'encoder.json'), 'r') as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join('gpt-2/models', model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "    return Encoder(\n",
        "        encoder=encoder,\n",
        "        bpe_merges=bpe_merges,\n",
        "    )\n",
        "\n",
        "def encode(dataX, model_name='117M'):\n",
        "    enc = get_encoder(model_name)\n",
        "    codeX = []\n",
        "    for i, x in enumerate(dataX):\n",
        "        vec = enc.encode(x)\n",
        "        codeX.append(vec)\n",
        "        \n",
        "    return sorted(codeX, key=len)\n",
        "\n",
        "\n",
        "dataX = encode(dataX)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rNROgCylCWb",
        "colab_type": "code",
        "outputId": "2d55e392-8984-41e9-8588-dacc42c345dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "seed=None\n",
        "\n",
        "def load_model(model_name='117M'):\n",
        "\n",
        "    with open(os.path.join('gpt-2/models', model_name, 'hparams.json')) as f:\n",
        "        hparams = model.default_hparams()\n",
        "        hparams.override_from_dict(json.load(f))   \n",
        "        def step(hparams, tokens, past=None):\n",
        "            lm_output = small_model(hparams=hparams, X=tokens, past=None, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "            h = lm_output['h']\n",
        "            return {\n",
        "                'h': h\n",
        "            }\n",
        "        \n",
        "        X = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        outputs = step(hparams, X) # (batch, sequence, embedding)\n",
        "        \n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join('gpt-2/models', model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "        \n",
        "    return X, outputs\n",
        "\n",
        "def add_binary_finetune(outputs):\n",
        "    with tf.variable_scope('binary_finetune'):\n",
        "        \n",
        "        final_embd = tf.math.l2_normalize(outputs['h'][:,-1,:], axis=-1)\n",
        "        w = tf.get_variable('w', [final_embd.shape[-1],1],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
        "        b = tf.get_variable('b', [final_embd.shape[0],1], initializer=tf.constant_initializer(0))\n",
        "        sess.run(tf.variables_initializer([w,b]))\n",
        "        ypred = tf.tanh(tf.matmul(final_embd,w, name='z') + b)\n",
        "        \n",
        "        ytrue = tf.placeholder(tf.float32, [batch_size, 1])\n",
        "        incorrects = tf.not_equal(tf.sign(ytrue), tf.sign(ypred), name='incorrects')\n",
        "        incorrects = tf.stop_gradient(tf.cast(incorrects, tf.float32))\n",
        "        \n",
        "        loss = tf.math.reduce_mean(tf.math.square((ypred - ytrue) * incorrects), name='loss')\n",
        "        \n",
        "        global_step = tf.Variable(0, trainable=False)\n",
        "        starter_learning_rate = 0.01\n",
        "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
        "                                           1000, 0.98, staircase=True)\n",
        "        optim = tf.train.AdamOptimizer(learning_rate=.0003)\n",
        "        minimize = optim.minimize(loss, global_step=global_step)\n",
        "        sess.run(tf.variables_initializer(optim.variables() + [global_step]))\n",
        "        \n",
        "    return ytrue, ypred, minimize, optim\n",
        "    \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
        "\n",
        "\n",
        "def get_train_infer(X, outputs, ytrue, ypred, minimize):\n",
        "    wt = sess.graph.get_tensor_by_name('binary_finetune/w:0')\n",
        "    zt = zt = sess.graph.get_tensor_by_name('binary_finetune/z:0')\n",
        "    losst = sess.graph.get_tensor_by_name('binary_finetune/loss:0')\n",
        "    incorrectst = sess.graph.get_tensor_by_name('binary_finetune/incorrects:0')\n",
        "    def train(dataX, labels):\n",
        "        for i in range(0, len(dataX), batch_size):\n",
        "            xfeed = pad_sequences(dataX[i:i+batch_size])\n",
        "            yfeed = labels[i:i+batch_size]\n",
        "            _, loss, pd, w, z, incorrects = sess.run([minimize,losst, ypred, wt, zt, incorrectst], \n",
        "                          options=run_options, feed_dict={X:xfeed, ytrue:yfeed})\n",
        "            if (i//batch_size) % 1000 == 0:\n",
        "                gc.collect()\n",
        " \n",
        "\n",
        "    def infer(dataX):\n",
        "        preds = []\n",
        "        for i in range(0, len(dataX), batch_size):\n",
        "            xfeed = pad_sequences(dataX[i:i+batch_size])\n",
        "            pd, w, z = sess.run([ypred, wt, zt], options=run_options, feed_dict={X: xfeed})\n",
        "            preds.append(pd)\n",
        "            if (i//batch_size) % 1000 == 0:\n",
        "                gc.collect()\n",
        "                print()\n",
        "                \n",
        "        preds = np.concatenate(preds, axis=0)\n",
        "        return preds\n",
        "\n",
        "    return train, infer\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ZIs3E3jsmvo",
        "colab_type": "code",
        "outputId": "175a8c73-3fff-4dc1-edce-a111b2c73d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(graph=tf.Graph(),config=config)\n",
        "sess.__enter__()\n",
        "X, outputs = load_model()\n",
        "ytrue, ypred, minimize, optim = add_binary_finetune(outputs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from gpt-2/models/117M/model.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yXJjkkxauDrt",
        "colab_type": "code",
        "outputId": "9728412d-7a4f-43f6-e24b-df52b4d247c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "out = open('gptransfer/results/out.txt', 'w')\n",
        "printout = lambda w: out.write(w + '\\n')\n",
        "\n",
        "def evaluate(pred, labels):\n",
        "    _, _, f1, _ = precision_recall_fscore_support(labels, pred)\n",
        "    return f1\n",
        "\n",
        "\n",
        "train, infer = get_train_infer(X, outputs, ytrue, ypred, minimize)\n",
        "\n",
        "eval_batchsize = 10000\n",
        "for i in range(0, 100000, eval_batchsize):\n",
        "    xbatch = dataX[i:i+eval_batchsize]\n",
        "    ybatch = labels[i:i+eval_batchsize]\n",
        "    train(xbatch, ybatch)\n",
        "    predbatch = np.sign(infer(xbatch))\n",
        "\n",
        "    f1 = evaluate(predbatch, ybatch)\n",
        "    print(f'f1 score: {f1}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "f1 score: [0.00401365 0.66950789]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}