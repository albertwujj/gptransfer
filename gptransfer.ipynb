{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gptransfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertwujj/gptransfer/blob/master/gptransfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ED518AlnbHEb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from os.path import join\n",
        "from google.colab import drive\n",
        "\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT, force_remount=True)\n",
        "\n",
        "PROJ = \"My Drive/ColabExperiments/\"\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "%cd \"{PROJECT_PATH}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tjHg2j9jkpVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from importlib.machinery import SourceFileLoader\n",
        "model = SourceFileLoader('model', 'gpt-2/src/model.py').load_module()\n",
        "encoder = SourceFileLoader('encoder', 'gpt-2/src/encoder.py').load_module()\n",
        "sample = SourceFileLoader('sample', 'gpt-2/src/sample.py').load_module()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GkMT9lW_ICKA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "np.set_printoptions(precision=3,threshold=sys.maxsize)\n",
        "import gc\n",
        "\n",
        "from itertools import islice\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "seed = 1957\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yzC4ZoGYYOjw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prepare data cell\n",
        "\n",
        "batch_size = 20\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from encoder import Encoder\n",
        "\n",
        "def read_amazon_kaggle(filename, lines=None):\n",
        "    with open(filename) as f:\n",
        "        y_x = [(1 if line[9] == '2' else 0, line[10:].strip()) for line in f]\n",
        "        if lines:\n",
        "            y_x = random.sample(y_x, lines)\n",
        "        y, x = zip(*y_x)\n",
        "        return x, y\n",
        "\n",
        "def get_encoder(model_name):\n",
        "    with open(os.path.join('gpt-2/models', model_name, 'encoder.json'), 'r') as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join('gpt-2/models', model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "    return Encoder(\n",
        "        encoder=encoder,\n",
        "        bpe_merges=bpe_merges,\n",
        "    )\n",
        "\n",
        "def encode(x, y, model_name='117M'):\n",
        "    enc = get_encoder(model_name)\n",
        "    xcode = []\n",
        "    for i, x in enumerate(x):\n",
        "        vec = enc.encode(x)\n",
        "        xcode.append(vec)\n",
        "    x = xcode\n",
        "    return zip(*sorted(zip(x, y), key = lambda t: len(t[0])))\n",
        "      \n",
        "def get_data(filename, total=None):\n",
        "    x_all, y_all = read_amazon_kaggle(filename, total)\n",
        "    x_all, y_all = encode(x_all, y_all)\n",
        "    x_all, y_all = (np.asarray(x_all), np.asarray(y_all))\n",
        "    return x_all, y_all\n",
        "\n",
        "def mod_by_batchsize(x):\n",
        "    l = (len(x) // batch_size) * batch_size\n",
        "    x = x[:l]\n",
        "    return x\n",
        "    \n",
        "test_size = 50000\n",
        "train_size = 20000\n",
        "val_size = 2500\n",
        "train_val_size = train_size + val_size\n",
        "\n",
        "x_all, y_all = get_data('gptransfer/data/amazonreviews/train.ft.txt', test_size+train_size+val_size)\n",
        "\n",
        "x_train, x_test, y_train, y_test =  train_test_split(x_all, y_all, test_size=test_size,random_state=seed)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size,random_state=seed)\n",
        "\n",
        "x_train, x_val, x_test, y_train, y_val, y_test = tuple(mod_by_batchsize(x) for x in (x_train, x_val, x_test, y_train, y_val, y_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYYhzjFB3VGk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyper Param definition cell\n",
        "frozen_layers = 8 # how many GPT2 layers to not train (out of 12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3y31vSJDdqbU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This cell contains all code copied from GPT2. Original GPT2 model with slight modifications.\n",
        "\n",
        "from model import shape_list, positions_for, norm, default_hparams, attn, gelu, conv1d\n",
        "\n",
        "dropout_prob = None\n",
        "def mlp(x, layer_num, scope, n_state, *, hparams):\n",
        "    with tf.variable_scope(scope):\n",
        "        nx = x.shape[-1].value\n",
        "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
        "        h2 = conv1d(h, 'c_proj', nx)\n",
        "        if layer_num > frozen_layers:\n",
        "            h2 = tf.nn.dropout(h2, rate=dropout_prob)\n",
        "        return h2\n",
        "    \n",
        "def block(x, layer_num, scope, *, past, hparams):\n",
        "    with tf.variable_scope(scope):\n",
        "        nx = x.shape[-1].value\n",
        "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
        "        x = x + a\n",
        "        m = mlp(norm(x, 'ln_2'), layer_num, 'mlp', nx*4, hparams=hparams)\n",
        "        x = x + m\n",
        "        return x, present\n",
        "    \n",
        "def custom_model(hparams, X, past=None, scope='model', reuse=False):\n",
        "    global dropout_prob\n",
        "    dropout_prob = tf.placeholder_with_default(0.0, shape=())\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        results = {}\n",
        "        batch, sequence = shape_list(X)\n",
        "\n",
        "        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
        "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
        "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
        "        \n",
        "        # Transformer\n",
        "        presents = []\n",
        "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
        "        assert len(pasts) == hparams.n_layer\n",
        "        for layer, past in enumerate(islice(pasts, hparams.n_layer)):\n",
        "            h, present = block(h, layer, 'h%d' % layer, past=past, hparams=hparams)\n",
        "            if layer == frozen_layers:\n",
        "                h = tf.stop_gradient(h)\n",
        "            presents.append(present)\n",
        "        results['present'] = tf.stack(presents, axis=1)\n",
        "        h = norm(h, 'ln_f')\n",
        "\n",
        "        # Language model loss.  Do tokens <n predict token n?\n",
        "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
        "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
        "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
        "        results['logits'] = logits\n",
        "\n",
        "        results['h'] = h\n",
        "        return results\n",
        " \n",
        "def small_model(hparams, X, **kwargs):\n",
        "    hparams.n_layer = 5\n",
        "    return custom_model(hparams, X, **kwargs)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rNROgCylCWb",
        "colab_type": "code",
        "outputId": "7b9e9bf7-e08e-43ea-e967-dcacea8383e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
        "\n",
        "def load_model(model_name='117M'):\n",
        "\n",
        "    with open(os.path.join('gpt-2/models', model_name, 'hparams.json')) as f:\n",
        "        hparams = model.default_hparams()\n",
        "        hparams.override_from_dict(json.load(f))   \n",
        "        def step(hparams, tokens, past=None):\n",
        "            lm_output = custom_model(hparams=hparams, X=tokens, past=None, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "            h = lm_output['h']\n",
        "            return {\n",
        "                'h': h\n",
        "            }\n",
        "        \n",
        "        X = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        outputs = step(hparams, X) # (batch, sequence, embedding)\n",
        "        \n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join('gpt-2/models', model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "        \n",
        "    return X, outputs\n",
        "\n",
        "def add_binary_finetune(outputs):\n",
        "    with tf.variable_scope('binary_finetune'):\n",
        "        \n",
        "        final_embd = outputs['h'][:,-1,:]\n",
        "        w = tf.get_variable('w', (final_embd.shape[-1],), initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
        "        b = tf.get_variable('b', (final_embd.shape[0],), initializer=tf.constant_initializer(0))\n",
        "        sess.run(tf.variables_initializer([w,b]))\n",
        "        l2loss = tf.nn.l2_loss(w)\n",
        "        logits = tf.tensordot(final_embd,w, [[1],[0]], name='z') + b\n",
        "        ypred = tf.tanh(logits)\n",
        "        \n",
        "        ytrue = tf.placeholder(tf.float32, (batch_size,))\n",
        "        incorrects = tf.not_equal(tf.sign(ytrue), tf.sign(ypred), name='incorrects')\n",
        "        incorrects = tf.stop_gradient(tf.cast(incorrects, tf.float32))\n",
        "        \n",
        "       \n",
        "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=ytrue, logits=logits) + l2loss * .01\n",
        "        \n",
        "        global_step = tf.Variable(0, trainable=False)\n",
        "        starter_learning_rate = 0.001\n",
        "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
        "                                           10000, 0.95, staircase=False)\n",
        "        optim = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        minimize = optim.minimize(loss, global_step=global_step)\n",
        "        sess.run(tf.variables_initializer(optim.variables() + [global_step]))\n",
        "        \n",
        "    return ytrue, ypred, minimize, optim\n",
        "\n",
        "\n",
        "def get_train_infer(X, outputs, ytrue, ypred, minimize):\n",
        "    \n",
        "    def train(dataX, labels):\n",
        "        for i in range(0, len(dataX), batch_size):\n",
        "            xfeed = pad_sequences(dataX[i:i+batch_size])\n",
        "            yfeed = labels[i:i+batch_size]\n",
        "            sess.run(minimize, options=run_options, feed_dict={X:xfeed, ytrue:yfeed, dropout_prob: 0.5})\n",
        "\n",
        "    def infer(dataX):\n",
        "        preds = []\n",
        "        for i in range(0, len(dataX), batch_size):\n",
        "            xfeed = pad_sequences(dataX[i:i+batch_size])\n",
        "            predbatch = sess.run(ypred, options=run_options, feed_dict={X: xfeed})\n",
        "            preds.append(predbatch)\n",
        "                \n",
        "        preds = np.concatenate(preds, axis=0)\n",
        "        return preds\n",
        "\n",
        "    return train, infer\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ww7MCMY0KbPg",
        "colab_type": "code",
        "outputId": "807c009d-4594-4b9d-9475-7d552634c70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.count_nonzero(np.sign(y_test - .5) == np.sign(pred_test - .5)) / y_test.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9246\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}