{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gptransfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertwujj/gptransfer/blob/master/gptransfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ED518AlnbHEb",
        "colab_type": "code",
        "outputId": "eb49cb13-da9e-4230-bd74-e12fffd6aae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "from os.path import join\n",
        "from google.colab import drive\n",
        "\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT, force_remount=True)\n",
        "\n",
        "PROJ = \"My Drive/ColabExperiments/\"\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "%cd \"{PROJECT_PATH}\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/ColabExperiments\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qNABxd2nO43O",
        "colab_type": "code",
        "outputId": "bd0f7979-c36f-4474-ef2d-8f5e1031b878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpt-2  gptransfer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tjHg2j9jkpVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from importlib.machinery import SourceFileLoader\n",
        "model = SourceFileLoader('model', 'gpt-2/src/model.py').load_module()\n",
        "encoder = SourceFileLoader('encoder', 'gpt-2/src/encoder.py').load_module()\n",
        "sample = SourceFileLoader('sample', 'gpt-2/src/sample.py').load_module()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GkMT9lW_ICKA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "from itertools import islice\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "seed = 1957\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3y31vSJDdqbU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from model import block, shape_list, positions_for, norm, default_hparams\n",
        "\n",
        "def custom_model(hparams, X, past=None, scope='model', reuse=False):\n",
        "\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        results = {}\n",
        "        batch, sequence = shape_list(X)\n",
        "\n",
        "        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
        "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
        "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
        "        \n",
        "        # Transformer\n",
        "        presents = []\n",
        "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
        "        assert len(pasts) == hparams.n_layer\n",
        "        for layer, past in enumerate(islice(pasts, hparams.n_layer)):\n",
        "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
        "            presents.append(present)\n",
        "        results['present'] = tf.stack(presents, axis=1)\n",
        "        h = norm(h, 'ln_f')\n",
        "\n",
        "        # Language model loss.  Do tokens <n predict token n?\n",
        "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
        "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
        "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
        "        results['logits'] = logits\n",
        "\n",
        "        results['h'] = h\n",
        "        return results\n",
        " \n",
        "def small_model(hparams, X, **kwargs):\n",
        "    hparams.n_layer = 5\n",
        "    return custom_model(hparams, X, **kwargs)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yzC4ZoGYYOjw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from encoder import Encoder\n",
        "\n",
        "def read_amazon_kaggle(filename, lines=None):\n",
        "    with open(filename) as f:\n",
        "        y_x = [(1 if line[9] == '2' else -1, line[10:].strip()) for line in f]\n",
        "        if lines:\n",
        "            y_x = random.sample(y_x, lines)\n",
        "        y, x = zip(*y_x)\n",
        "        return x, y\n",
        "\n",
        "def get_encoder(model_name):\n",
        "    with open(os.path.join('gpt-2/models', model_name, 'encoder.json'), 'r') as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join('gpt-2/models', model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "    return Encoder(\n",
        "        encoder=encoder,\n",
        "        bpe_merges=bpe_merges,\n",
        "    )\n",
        "\n",
        "def encode(x, y, model_name='117M'):\n",
        "    enc = get_encoder(model_name)\n",
        "    xcode = []\n",
        "    for i, x in enumerate(x):\n",
        "        vec = enc.encode(x)\n",
        "        xcode.append(vec)\n",
        "    x = xcode\n",
        "    return zip(*sorted(zip(x, y), key = lambda t: len(t[0])))\n",
        "      \n",
        "\n",
        "x_all, y_all = read_amazon_kaggle('gptransfer/data/amazonreviews/train.ft.txt', 100000)\n",
        "x_all, y_all = encode(x_all, y_all)\n",
        "x_all, y_all = (np.asarray(x_all), np.asarray(y_all))\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_all, y_all, test_size=0.04,random_state=seed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rNROgCylCWb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "def load_model(model_name='117M'):\n",
        "\n",
        "    with open(os.path.join('gpt-2/models', model_name, 'hparams.json')) as f:\n",
        "        hparams = model.default_hparams()\n",
        "        hparams.override_from_dict(json.load(f))   \n",
        "        def step(hparams, tokens, past=None):\n",
        "            lm_output = custom_model(hparams=hparams, X=tokens, past=None, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "            h = lm_output['h']\n",
        "            return {\n",
        "                'h': h\n",
        "            }\n",
        "        \n",
        "        X = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        outputs = step(hparams, X) # (batch, sequence, embedding)\n",
        "        \n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join('gpt-2/models', model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "        \n",
        "    return X, outputs\n",
        "\n",
        "def add_binary_finetune(outputs):\n",
        "    with tf.variable_scope('binary_finetune'):\n",
        "        \n",
        "        final_embd = tf.math.l2_normalize(outputs['h'][:,-1,:], axis=-1)\n",
        "        w = tf.get_variable('w', (final_embd.shape[-1],), initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
        "        b = tf.get_variable('b', (final_embd.shape[0],), initializer=tf.constant_initializer(0))\n",
        "        sess.run(tf.variables_initializer([w,b]))\n",
        "        ypred = tf.tanh(tf.tensordot(final_embd,w, [[1],[0]], name='z') + b)\n",
        "        \n",
        "        ytrue = tf.placeholder(tf.float32, (batch_size,))\n",
        "        incorrects = tf.not_equal(tf.sign(ytrue), tf.sign(ypred), name='incorrects')\n",
        "        incorrects = tf.stop_gradient(tf.cast(incorrects, tf.float32))\n",
        "        \n",
        "        loss = tf.math.reduce_mean(tf.math.square((ypred - ytrue) * incorrects), name='loss')\n",
        "        \n",
        "        global_step = tf.Variable(0, trainable=False)\n",
        "        starter_learning_rate = 0.0003\n",
        "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
        "                                           500, 0.94, staircase=False)\n",
        "        optim = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        minimize = optim.minimize(loss, global_step=global_step)\n",
        "        sess.run(tf.variables_initializer(optim.variables() + [global_step]))\n",
        "        \n",
        "    return ytrue, ypred, minimize, optim\n",
        "\n",
        "\n",
        "def get_train_infer(X, outputs, ytrue, ypred, minimize):\n",
        "    \n",
        "    def train(dataX, labels):\n",
        "        for i in range(0, len(dataX), batch_size):\n",
        "            xfeed = pad_sequences(dataX[i:i+batch_size])\n",
        "            yfeed = labels[i:i+batch_size]\n",
        "            sess.run(minimize, options=run_options, feed_dict={X:xfeed, ytrue:yfeed})\n",
        "\n",
        "    def infer(dataX):\n",
        "        preds = []\n",
        "        for i in range(0, len(dataX), batch_size):\n",
        "            xfeed = pad_sequences(dataX[i:i+batch_size])\n",
        "            predbatch = sess.run(ypred, options=run_options, feed_dict={X: xfeed})\n",
        "            preds.append(predbatch)\n",
        "                \n",
        "        preds = np.concatenate(preds, axis=0)\n",
        "        return preds\n",
        "\n",
        "    return train, infer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yXJjkkxauDrt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.contrib.memory_stats.python.ops.memory_stats_ops import MaxBytesInUse\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    with tf.Session(graph=tf.Graph(),config=config) as sess:\n",
        "        max_bytes_in_use = MaxBytesInUse()\n",
        "\n",
        "        X, outputs = load_model()\n",
        "        ytrue, ypred, minimize, optim = add_binary_finetune(outputs)\n",
        "        train, infer = get_train_infer(X, outputs, ytrue, ypred, minimize)\n",
        "           \n",
        "        eval_batchsize = 500 * batch_size\n",
        "        start = timer()\n",
        "        with open('gptransfer/results/out', 'w', buffering=1) as out:\n",
        "            for i in range(0, x_train.shape[0], eval_batchsize):\n",
        "\n",
        "                end = min(x_train.shape[0], i+eval_batchsize)\n",
        "                x_train_b = x_train[i:end]\n",
        "                y_train_b = y_train[i:end]\n",
        "                train(x_train_b, y_train_b)\n",
        "\n",
        "                pred_val = infer(x_val)\n",
        "                roc_score = roc_auc_score(y_val, pred_val)\n",
        "                corrects = np.count_nonzero(y_val == np.sign(pred_val)) / y_val.shape[0]\n",
        "\n",
        "                gb_used = sess.run(max_bytes_in_use) / 1e9\n",
        "                print(f'roc score: {roc_score}')\n",
        "                print(f'corrects: {corrects}')\n",
        "\n",
        "                printout = lambda w: out.write(w + '\\n')\n",
        "                printout(f'roc score {i//eval_batchsize}: {roc_score:.3f}')\n",
        "                printout(f'corrects {i//eval_batchsize}: {corrects:.3f}')\n",
        "                printout(f'GPU GB used {i//eval_batchsize}: {gb_used:.2f}')\n",
        "\n",
        "                print(f'GPU GB used {i//eval_batchsize}: {gb_used:.2f}')\n",
        "                print(f'elapsed minutes {i//eval_batchsize}: {(timer() - start)//60}')\n",
        "                \n",
        "                %cp gptransfer/results/out gptransfer/results/out_backup\n",
        " \n",
        "\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}