{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpshare.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertwujj/gptransfer/blob/master/gpshare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "MdvnxlSOBkxS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MOUNT GOOGLE DRIVE\n",
        "# creates a new folder called 'ColabExperiments1921'\n",
        "\n",
        "# I put the final embedding output through a fully-connected layer and train on binary cross-entropy,\n",
        "# and freeze the earlier layers of the decoder and add regularization.\n",
        "\n",
        "import os\n",
        "from os.path import join\n",
        "from google.colab import drive\n",
        "\n",
        "%cd /\n",
        "\n",
        "ROOT = '/content/drive1921/'\n",
        "if not os.path.exists(ROOT):\n",
        "    drive.mount(ROOT)\n",
        "%cd '{ROOT}'\n",
        "\n",
        "PROJ = 'My Drive/ColabExperiments1921/'\n",
        "if not os.path.exists(PROJ):\n",
        "    !mkdir '{PROJ}'\n",
        "%cd '{PROJ}'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KfcQmoi0ZXgr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download data+code\n",
        "if not os.path.exists('lmtransfer'):\n",
        "    !gdown https://drive.google.com/uc?id=1VxxPnBOLh7a0kl6XCUFundH4yD-McWRu\n",
        "    !unzip gpshare.zip\n",
        "    !rm gpshare.zip\n",
        "%cd lmtransfer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JyUmtPnzMcdM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setup code modules and path\n",
        "if not os.path.exists('src'):\n",
        "    !mkdir src\n",
        "%cd src\n",
        "\n",
        "from importlib.machinery import SourceFileLoader\n",
        "path = '../input/'\n",
        "model = SourceFileLoader('model', path+'gpt-2/src/model.py').load_module()\n",
        "encoder = SourceFileLoader('encoder', path+'gpt-2/src/encoder.py').load_module()\n",
        "sample = SourceFileLoader('sample', path+'gpt-2/src/sample.py').load_module()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "DuEDd0HxBckj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import sys\n",
        "from sklearn import metrics\n",
        "np.set_printoptions(precision=3,threshold=sys.maxsize)\n",
        "import gc\n",
        "\n",
        "from itertools import islice\n",
        "import json\n",
        "import random\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "\n",
        "seed = None\n",
        "random.seed(1957)\n",
        "np.random.seed(1957)\n",
        "tf.set_random_seed(1957)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yzC4ZoGYYOjw",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "\n",
        "batch_size = 10\n",
        "clf_token = None\n",
        "from encoder import Encoder\n",
        "\n",
        "\n",
        "def read_amazon_kaggle(lines=None, test=False):\n",
        "    dataset = 'test' if test else 'train'\n",
        "    with open(f'../input/amazonreviews/{dataset}.ft.txt') as f:\n",
        "        y_x = [(1 if line[9] == '2' else 0, line[10:].strip()) for line in f]\n",
        "        if lines:\n",
        "            y_x = random.sample(y_x, lines)\n",
        "        y, x = zip(*y_x)\n",
        "        return x, y, np.ones((len(x),)), np.zeros((len(y),1)) # last 2 for compatability with other dataset I'm testing on\n",
        "\n",
        "def get_encoder(model_name):\n",
        "    global clf_token\n",
        "    with open(os.path.join(path, 'gpt-2/models', model_name, 'encoder.json'), 'r') as f:\n",
        "        encoder = json.load(f)\n",
        "    with open(os.path.join(path, 'gpt-2/models', model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
        "        bpe_data = f.read()\n",
        "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
        "    return Encoder(\n",
        "        encoder=encoder,\n",
        "        bpe_merges=bpe_merges,\n",
        "    )\n",
        "\n",
        "def encode(datapack, model_name='117M'):\n",
        "    enc = get_encoder(model_name)\n",
        "    xcode = []\n",
        "    for i, x in enumerate(datapack[0]):\n",
        "        vec = enc.encode(x) + [enc.encoder['________________________________________________________________']]\n",
        "        xcode.append(vec)\n",
        "    datapack = (np.asarray(xcode),) + datapack[1:]\n",
        "    return [np.asarray(x) for x in zip(*sorted(zip(*datapack), key = lambda t: len(t[0])))]\n",
        "      \n",
        "\n",
        "def mod_by_batchsize(x):\n",
        "    l = (len(x) // batch_size) * batch_size\n",
        "    x = x[:l]\n",
        "    return x\n",
        "\n",
        "\n",
        "def read_test(lines=None):\n",
        "    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
        "    df_submit = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n",
        "    if lines:\n",
        "        test = test.sample(lines)\n",
        "        df_submit = df_submit.sample(lines)\n",
        "    x, inds = encode(([x.strip('\\\"') for x in test['comment_text'].tolist()],test['id'].tolist()))\n",
        "    return x, inds, df_submit    \n",
        "\n",
        "\n",
        "lines = 30000\n",
        "datapack = read_amazon_kaggle(lines=lines)\n",
        "x_all, y_all, weights, identities = encode(datapack)\n",
        "x_test, y_test = encode(read_amazon_kaggle(test=True)[:2])\n",
        "print(len(x_test))\n",
        "weights /= np.mean(weights)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-uC29GiQYpMu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Global Hyper Param definition \n",
        "\n",
        "frozen_blocks = None\n",
        "lr = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3y31vSJDdqbU",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setup OpenAI GPT-2 for training (add dropout)\n",
        "\n",
        "from model import *\n",
        "import model\n",
        "\n",
        "is_train = None\n",
        "curr_layer = None\n",
        "sess = None\n",
        "\n",
        "def dropout(x, rate=.1, noise_shape=None):\n",
        "    return tf.nn.dropout(x, rate=rate*is_train, noise_shape=noise_shape)\n",
        "\n",
        "def attn(x, scope, n_state, *, past, hparams):\n",
        "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
        "    assert n_state % hparams.n_head == 0\n",
        "    if past is not None:\n",
        "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
        "\n",
        "    def split_heads(x):\n",
        "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
        "        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n",
        "\n",
        "    def merge_heads(x):\n",
        "        # Reverse of split_heads\n",
        "        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
        "\n",
        "    def mask_attn_weights(w):\n",
        "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
        "        _, _, nd, ns = shape_list(w)\n",
        "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
        "        b = tf.reshape(b, [1, 1, nd, ns])\n",
        "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
        "        return w\n",
        "\n",
        "    def multihead_attn(q, k, v):\n",
        "        # q, k, v have shape [batch, heads, sequence, features]\n",
        "        w = tf.matmul(q, k, transpose_b=True)\n",
        "        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n",
        "\n",
        "        w = mask_attn_weights(w)\n",
        "        w = softmax(w)\n",
        "        if curr_layer not in frozen_blocks:\n",
        "            w = dropout(w)\n",
        "        a = tf.matmul(w, v)\n",
        "        return a\n",
        "\n",
        "    with tf.variable_scope(scope):\n",
        "        c = conv1d(x, 'c_attn', n_state*3)\n",
        "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
        "        present = tf.stack([k, v], axis=1)\n",
        "        if past is not None:\n",
        "            pk, pv = tf.unstack(past, axis=1)\n",
        "            k = tf.concat([pk, k], axis=-2)\n",
        "            v = tf.concat([pv, v], axis=-2)\n",
        "        a = multihead_attn(q, k, v)\n",
        "        a = merge_heads(a)\n",
        "        a = conv1d(a, 'c_proj', n_state)\n",
        "        if curr_layer not in frozen_blocks:\n",
        "            a = dropout(a)\n",
        "        return a, present\n",
        "\n",
        "\n",
        "def mlp(x, scope, n_state, *, hparams):\n",
        "    with tf.variable_scope(scope):\n",
        "        nx = x.shape[-1].value\n",
        "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
        "        h2 = conv1d(h, 'c_proj', nx)\n",
        "        if curr_layer not in frozen_blocks:\n",
        "            h2 = dropout(h2)\n",
        "        return h2\n",
        "    \n",
        "model.mlp = mlp\n",
        "model.attn = attn\n",
        "\n",
        "\n",
        "\n",
        "frozen_layer = 5\n",
        "def custom_model(hparams, X, past=None, scope='model', reuse=False):\n",
        "    global is_train, curr_layer\n",
        "    is_train = tf.placeholder_with_default(0.0, shape=())\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        results = {}\n",
        "        batch, sequence = shape_list(X)\n",
        "\n",
        "        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.01))\n",
        "        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
        "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
        "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
        "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
        "        \n",
        "        # Transformer\n",
        "        presents = []\n",
        "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
        "        assert len(pasts) == hparams.n_layer\n",
        "        for layer, past in enumerate(islice(pasts, hparams.n_layer)):\n",
        "            curr_layer = layer\n",
        "            if curr_layer >= frozen_layer: \n",
        "                h = tf.stop_gradient(h)\n",
        "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
        "\n",
        "            presents.append(present)\n",
        "        results['present'] = tf.stack(presents, axis=1)\n",
        "        h = norm(h, 'ln_f')\n",
        "\n",
        "        # Language model loss.  Do tokens <n predict token n?\n",
        "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
        "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
        "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
        "        results['logits'] = logits\n",
        "\n",
        "        results['h'] = h\n",
        "        \n",
        "        results['wte'] = wte\n",
        "        return results\n",
        " \n",
        "def small_model(hparams, X, **kwargs):\n",
        "    hparams.n_layer = 5\n",
        "    return custom_model(hparams, X, **kwargs)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7oVg4u6LYkoA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Add fine-tuning\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "hparams = None\n",
        "def load_model(model_name='117M'):\n",
        "    global hparams\n",
        "    with open(os.path.join(path, 'gpt-2/models', model_name, 'hparams.json')) as f:\n",
        "        hparams = model.default_hparams()\n",
        "        hparams.override_from_dict(json.load(f))   \n",
        "        def step(hparams, tokens, past=None):\n",
        "            lm_output = custom_model(hparams=hparams, X=tokens, past=None, reuse=tf.AUTO_REUSE)\n",
        "            return lm_output\n",
        "        \n",
        "        X = tf.placeholder(tf.int32, [None, None])\n",
        "        outputs = step(hparams, X) # (batch, sequence, embedding)\n",
        "        \n",
        "        saver = tf.train.Saver(var_list=[v for v in tf.trainable_variables() if 'lm_h' not in v.name])\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(path, 'gpt-2/models', model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "        \n",
        "    return X, outputs\n",
        "weights_t = None\n",
        "\n",
        "lstm_o1 = tf.keras.layers.CuDNNLSTM(500,time_major=False,return_sequences=True)\n",
        "\n",
        "\n",
        "def add_binary_finetune(X, outputs):\n",
        "    global weights_t\n",
        "    with tf.variable_scope('binary_finetune'):\n",
        "            \n",
        "       \n",
        "        h = outputs['h']\n",
        "        wte = outputs['wte']\n",
        "        \n",
        "        # also train on LM objective\n",
        "        lm_h = outputs['h']\n",
        "        lm_h = tf.reshape(lm_h[:, :-1, :], [-1, hparams.n_embd])\n",
        "        lm_logits = tf.matmul(lm_h, wte, transpose_b=True)\n",
        "        lm_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=lm_logits, labels=tf.reshape(X[:, 1:], [-1]))\n",
        "        lm_losses = tf.reduce_sum(tf.reshape(lm_losses, [shape_list(X)[0], shape_list(X)[1]-1]), 1)\n",
        "        \n",
        "        h = lstm_o1.apply(h)\n",
        "        h = dropout(h)\n",
        "        final_embd = h[:,-1,:] # (batch, n_embd)\n",
        "    \n",
        "        w = tf.get_variable('w', (final_embd.shape[-1],), initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
        "        b = tf.get_variable('b', (1,), initializer=tf.constant_initializer(0))\n",
        "\n",
        "\n",
        "        \n",
        "        l2loss = tf.nn.l2_loss(w)\n",
        "        logits = tf.tensordot(final_embd,w, [[1],[0]], name='z') + b\n",
        "        ypred = tf.nn.sigmoid(logits)\n",
        "        \n",
        "        ytrue = tf.placeholder(tf.float32, (None,))\n",
        "        \n",
        "        weights_t = tf.placeholder(tf.float32, (None,))\n",
        "        loss = tf.reduce_mean(.3 * lm_losses + weights_t * (tf.nn.sigmoid_cross_entropy_with_logits(labels=ytrue, logits=logits) + l2loss * .02))\n",
        "        \n",
        "        global_step = tf.Variable(0, trainable=False)\n",
        "        starter_learning_rate = lr\n",
        "        learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n",
        "                                           x_all.shape[0]*2//(batch_size), end_learning_rate=0.3,cycle=True)\n",
        "        optim = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "        \n",
        "        \n",
        "    return ytrue, ypred, optim, loss\n",
        "\n",
        "def get_train_infer(X, outputs, ytrue, ypred, minimize):\n",
        "    def train(dataX, labels, weights_b):\n",
        "        for i in range(0, len(dataX), batch_size):\n",
        "            xfeed = pad_sequences(dataX[i:i+batch_size])\n",
        "            yfeed = labels[i:i+batch_size]\n",
        "            weights_feed = weights_b[i:i+batch_size]\n",
        "           \n",
        "            sess.run(minimize, feed_dict={X:xfeed, ytrue:yfeed, weights_t:weights_feed, is_train: 1})\n",
        "\n",
        "    def infer(dataX):\n",
        "        preds = []\n",
        "        for i in range(0, len(dataX), batch_size):\n",
        "            xfeed = pad_sequences(dataX[i:i+batch_size])\n",
        "            predbatch = sess.run(ypred, feed_dict={X: xfeed})\n",
        "            preds.append(predbatch)\n",
        "                \n",
        "        preds = np.concatenate(preds, axis=0)\n",
        "        return preds\n",
        "\n",
        "    return train, infer\n",
        "from numba import cuda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vrW5Sb0oCUxQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "# Training and testing\n",
        "\n",
        "def run_infer(space):\n",
        "    global sess, lr, frozen_blocks\n",
        "    lr = space['lr']\n",
        "    frozen_blocks = space['frozen_blocks']\n",
        "    epochs = 2\n",
        "\n",
        "    with tf.device('/device:GPU:0') as dev, tf.Session(graph=tf.get_default_graph(),\n",
        "                                                   config=tf.ConfigProto(allow_soft_placement=True)) as sessl:\n",
        "        sess = sessl\n",
        "        X, outputs = load_model()\n",
        "        ytrue, ypred, optim, loss = add_binary_finetune(X, outputs)\n",
        "\n",
        "        train_vars = tf.trainable_variables()\n",
        "        train_cond = lambda v: v.name.split('/')[0] == 'binary_finetune' or (v.name[:7] == 'model/h' and int(v.name[7] not in frozen_blocks))    \n",
        "        train_vars = [v for v in train_vars if train_cond(v)]\n",
        "        print([v.name for v in train_vars])\n",
        "        minimize = optim.minimize(loss, var_list = train_vars)\n",
        "\n",
        "        need_init = set(s.decode(\"utf-8\") for s in sess.run(tf.report_uninitialized_variables()))\n",
        "        print(need_init)\n",
        "        init = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in need_init])\n",
        "        sess.run(init)\n",
        "\n",
        "        train, infer = get_train_infer(X, outputs, ytrue, ypred, minimize)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start = timer()\n",
        "            train(x_all, y_all, weights)\n",
        "            print(f'time elapsed for epoch {epoch}: {timer()-start}')\n",
        "            \n",
        "        pred_test = infer(x_test)\n",
        "        accuracy = np.count_nonzero(np.sign(y_test - .5) == np.sign(pred_test - .5)) / y_test.shape[0]\n",
        "        print(f'Testing accuracy: {accuracy}')\n",
        "        cuda.select_device(0)\n",
        "        cuda.close()\n",
        "\n",
        "run_infer({'lr': .0003, 'frozen_blocks':list(range(frozen_layer)) + []})\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UNFMRQIA9s93",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training and validation, spawning new processes to clear GPU mem\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from tensorflow.contrib.memory_stats.python.ops.memory_stats_ops import MaxBytesInUse\n",
        "from sklearn.model_selection import KFold as KFold\n",
        "from itertools import product\n",
        "import multiprocess as mp\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "\n",
        "import pickle\n",
        "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
        "\n",
        "from functools import reduce, partial\n",
        "import sklearn\n",
        "\n",
        "def loadTrials(trials_name): \n",
        "    trials = pickle.load(open(trials_name, \"rb\"))\n",
        "    # delete all 'running' jobs- jobs stopped in execution\n",
        "    trials._dynamic_trials = [t for t in trials._dynamic_trials if t['state'] != 1 and t['state'] != 3]\n",
        "    for i, t in enumerate(trials._dynamic_trials):\n",
        "        t['tid'] = i\n",
        "    trials._ids = set()\n",
        "    trials.refresh()\n",
        "    return trials\n",
        "\n",
        "epochs = 2\n",
        "                        \n",
        "def val(): \n",
        "    \n",
        "    with open('out', 'w') as out:\n",
        "        printout = lambda w: out.write(w+'\\n')\n",
        "        def objective(space):\n",
        "\n",
        "            global lr, frozen_blocks\n",
        "            lr = space['lr']\n",
        "            frozen_blocks = space['frozen_blocks']\n",
        "            hparams_string = reduce(lambda x,y:x+y,[f'{k} {v} ' for k, v in space.items()])\n",
        "            print(hparams_string)\n",
        "            printout(hparams_string)\n",
        "\n",
        "            kf = KFold(n_splits=3,random_state=None, shuffle=True)\n",
        "            scores = []\n",
        "            for train_index, val_index in islice(kf.split(x_all, y_all), 2):\n",
        "                x_train, weights_train, _, y_train, x_val, _, idens_val, y_val = [mod_by_batchsize(arr[inds]) for inds, arr in product([train_index, val_index], [x_all,weights,identities,y_all])]\n",
        "\n",
        "                def run_fold(acc):\n",
        "                    global sess\n",
        "                    with tf.device('/device:GPU:0') as dev, tf.Session(graph=tf.get_default_graph(),\n",
        "                                                                   config=tf.ConfigProto(allow_soft_placement=True)) as sess1:\n",
        "\n",
        "                        sess = sess1\n",
        "                        max_bytes_in_use = MaxBytesInUse()\n",
        "\n",
        "                        X, outputs = load_model()\n",
        "                        ytrue, ypred, optim, loss = add_binary_finetune(X, outputs)\n",
        "\n",
        "                        train_vars = tf.trainable_variables()\n",
        "                        train_cond = lambda v: v.name.split('/')[0] == 'binary_finetune' or (v.name[:7] == 'model/h' and int(v.name[7] not in frozen_blocks))    \n",
        "                        train_vars = [v for v in train_vars if train_cond(v)]\n",
        "                        print([v.name for v in train_vars])\n",
        "                        minimize = optim.minimize(loss, var_list = train_vars)\n",
        "                        \n",
        "                        need_init = set(s.decode(\"utf-8\") for s in sess.run(tf.report_uninitialized_variables()))\n",
        "                        print(need_init)\n",
        "                        init = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in need_init])\n",
        "                        sess.run(init)\n",
        "                        \n",
        "                        train, infer = get_train_infer(X, outputs, ytrue, ypred, minimize)\n",
        "\n",
        "                        for epoch in range(epochs):\n",
        "                            eval_batchsize = 500000\n",
        "                            eval_batchsize = (eval_batchsize//batch_size) * batch_size\n",
        "                            start = timer()\n",
        "                            \n",
        "                            for i in range(0, x_train.shape[0], eval_batchsize):\n",
        "                                end = min(x_train.shape[0], i+eval_batchsize)\n",
        "                                x_train_b = x_train[i:end]\n",
        "                                y_train_b = y_train[i:end]\n",
        "                                weights_train_b = weights_train[i:end]\n",
        "                                train(x_train_b, y_train_b, weights_train_b)\n",
        "\n",
        "                        pred_val = infer(x_val)\n",
        "                        accuracy = np.count_nonzero(np.sign(y_val - .5) == \n",
        "                                                    np.sign(pred_val - .5)) / y_val.shape[0]\n",
        "\n",
        "                        gb_used = sess.run(max_bytes_in_use) / 1e9\n",
        "                        print(f'GPU GB used: {gb_used:.2f}')\n",
        "                        print(f'elapsed minutes: {(timer() - start)//60}')\n",
        "\n",
        "          \n",
        "                        print('Fold Complete')\n",
        "                        print(f'Score: {accuracy}')\n",
        "                        printout(f'Score: {accuracy}')\n",
        "                        acc.value = accuracy\n",
        "                        cuda.select_device(0)\n",
        "                        cuda.close()\n",
        "\n",
        "                acc = mp.Value('f',0.0)\n",
        "                p = mp.Process(target=run_fold, args=(acc,))\n",
        "\n",
        "                p.start()\n",
        "                p.join()\n",
        "                if 0 and acc.value < .3: # too bad, don't do next\n",
        "                    return {'loss': -acc.value, 'status': STATUS_OK}\n",
        "                scores.append(acc.value)\n",
        "\n",
        "            avg_score = sum(scores) / len(scores)\n",
        "            print(f'Average score with {hparams_string}: {avg_score}\\n\\n')\n",
        "            %cp out backupout\n",
        "            return {'loss': -(avg_score), 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "\n",
        "        def random_search():\n",
        "            best_acc = None\n",
        "            frozen_blocks = set(list(range(3)))\n",
        "            start_lr = .0002\n",
        "            end_lr = .0006\n",
        "            for i in range(30):\n",
        "                num_frozen = random.randint(1,3)\n",
        "                frozen_blocks = frozen_blocks.union(random.sample(range(3, 8), 1))\n",
        "                lr = np.exp(random.uniform(np.log(start_lr), np.log(end_lr))).astype(np.float32)\n",
        "                space = {'lr': .0004, 'frozen_blocks':list(range(frozen_layer)) + []}\n",
        "                results = objective(space)\n",
        "\n",
        "                if best_acc is None or -results['loss'] > best_acc:\n",
        "                    best_acc = -results['loss']\n",
        "            print(f'BEST score: {best_acc}')\n",
        "        \n",
        "        \n",
        "        random_search()\n",
        "        #space = {'lr':.0001, 'frozen_blocks':list(range(8))}\n",
        "        #results = objective(space)\n",
        "    \n",
        "\n",
        "if __name__=='__main__':\n",
        "    pass\n",
        "    \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}